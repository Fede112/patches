{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 28/08/2019\n",
    "\n",
    "What follows is a simple implementation of a convolutional autoencoder using patches extracted from full image mammographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import utils\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mm_patch.data import PatchesDataset\n",
    "import mm_patch.transforms\n",
    "\n",
    "\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function\n",
    "def online_mean_and_std(loader):\n",
    "    \"\"\"Compute the mean and std for all pixels in an image.\n",
    "    It does not require to have all the dataset loaded in RAM \n",
    "    as it calculates it one batch at a time.\n",
    "\n",
    "        Var[x] = E[X^2] - E^2[X]\n",
    "        \n",
    "    ref: https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949/8\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    # Is there a better way of checking the # of channels?\n",
    "    channels = next(iter(loader))['patch'].shape[1]\n",
    "    fst_moment = torch.empty(channels)\n",
    "    snd_moment = torch.empty(channels)\n",
    "\n",
    "    for batch in loader:\n",
    "\n",
    "        b, c, h, w = batch['patch'].shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(batch['patch'].float(), dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(batch['patch'].float() ** 2, dim=[0, 2, 3])\n",
    "        fst_moment = (cnt * fst_moment + sum_) / (cnt + nb_pixels)\n",
    "        snd_moment = (cnt * snd_moment + sum_of_square) / (cnt + nb_pixels)\n",
    "\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    return fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "# First I create a dataloader to calculate mean and std of the pixels.\n",
    "# For the latter I will use `online_mean_and_std(loader)`\n",
    "patches = PatchesDataset(data_path='./patches.pkl', transform = mm_patch.transforms.ToTensor())\n",
    "loader = DataLoader(patches, batch_size=20, shuffle=False)\n",
    "mean_px, std_px = online_mean_and_std(loader)\n",
    "print(mean_px.dtype,std_px.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18968.8457]), tensor([7423.2759]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_px,std_px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating mean and std of the dataset, I can build a dataset with the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "composed = transforms.Compose([ \n",
    "#                                 mm_patch.transforms.ToImage(),\n",
    "                                mm_patch.transforms.ToTensor(),\n",
    "                                mm_patch.transforms.Scale(),\n",
    "#                                 mm_patch.transforms.GrayToRGB(),\n",
    "                                mm_patch.transforms.Normalize(mean=[mean_px], std=[std_px] )\n",
    "                            ])\n",
    "# Set dataset\n",
    "patches = PatchesDataset(data_path='./patches.pkl', transform = composed)\n",
    "\n",
    "# Dataloader parameters\n",
    "batch_size = 20\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(patches)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(patches, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=4)\n",
    "validation_loader = torch.utils.data.DataLoader(patches, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-13bb91273406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     print(i_batch, sample_batched['patch'].size(),\n\u001b[1;32m     32\u001b[0m           sample_batched['target'].size())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Helper function to show a batch\n",
    "def show_patches_batch(sample_batched, max_images = None):\n",
    "    \"\"\"Show patches for a batch of samples.\"\"\"\n",
    "    patches_batch, targets_batch = \\\n",
    "            sample_batched['patch'], sample_batched['target']\n",
    "    num_img = len(patches_batch)\n",
    "    \n",
    "    if max_images:\n",
    "        num_img = max_images\n",
    "    \n",
    "    sqrt_num_img = np.ceil(np.sqrt(num_img))\n",
    "    im_size = patches_batch.size(2)\n",
    "#     print(im_size)\n",
    "    grid_border_size = 2\n",
    "\n",
    "    grid = utils.make_grid(patches_batch)\n",
    "    \n",
    "#     plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "    fig = plt.figure(figsize = (10,8))\n",
    "    for i in range(batch_size):\n",
    "        ax = fig.add_subplot(sqrt_num_img, sqrt_num_img, i+1)\n",
    "        ax.imshow(patches_batch[i][0])\n",
    "        ax.set_title(f'Class: {patches.unique_labels[targets_batch[i]]}')\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "#         plt.title('Batch from dataloader')\n",
    "    plt.show()\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['patch'].size(),\n",
    "          sample_batched['target'].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        plt.figure()\n",
    "        show_patches_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "#### autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, width=13):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.width = width\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(224 * 224, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(64, self.width) )\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.width, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(128, 224 * 224), \n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x \n",
    "      \n",
    "    def encode(self,x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self,code):\n",
    "        return self.decoder(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=13\n",
    "model = autoencoder(width=width)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 128]       6,422,656\n",
      "              ReLU-2               [-1, 1, 128]               0\n",
      "            Linear-3                [-1, 1, 64]           8,256\n",
      "              ReLU-4                [-1, 1, 64]               0\n",
      "            Linear-5                [-1, 1, 13]             845\n",
      "            Linear-6                [-1, 1, 64]             896\n",
      "              ReLU-7                [-1, 1, 64]               0\n",
      "            Linear-8               [-1, 1, 128]           8,320\n",
      "              ReLU-9               [-1, 1, 128]               0\n",
      "           Linear-10             [-1, 1, 50176]       6,472,704\n",
      "             Tanh-11             [-1, 1, 50176]               0\n",
      "================================================================\n",
      "Total params: 12,913,677\n",
      "Trainable params: 12,913,677\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 0.77\n",
      "Params size (MB): 49.26\n",
      "Estimated Total Size (MB): 50.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_size = (1,50176,)\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convolutional autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = torch.sigmoid(self.t_conv2(x))\n",
    "                \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 224, 224]             160\n",
      "         MaxPool2d-2         [-1, 16, 112, 112]               0\n",
      "            Conv2d-3          [-1, 4, 112, 112]             580\n",
      "         MaxPool2d-4            [-1, 4, 56, 56]               0\n",
      "   ConvTranspose2d-5         [-1, 16, 112, 112]             272\n",
      "   ConvTranspose2d-6          [-1, 1, 224, 224]              65\n",
      "================================================================\n",
      "Total params: 1,077\n",
      "Trainable params: 1,077\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 10.05\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 10.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_size = (1,224,224)\n",
    "summary(model, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
