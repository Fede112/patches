{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from mm_patch.data import PatchesDataset\n",
    "import mm_patch.transforms\n",
    "from mm_patch.utils import image_from_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations to be compatible with Densenet-121 from NYU paper.\n",
    "# Note I am using mean and std as recommended in Pytorch. Maybe calculating the dataset statistics is better.\n",
    "composed = transforms.Compose([ \n",
    "#                                 mm_patch.transforms.ToImage(),\n",
    "                                transforms.ToTensor(),\n",
    "                                mm_patch.transforms.Scale(),\n",
    "#                                 mm_patch.transforms.GrayToRGB(),\n",
    "                                transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_png(file_name):\n",
    "    image = np.array(imageio.imread(file_name)).astype(np.int32)\n",
    "    return image\n",
    "\n",
    "patches = datasets.ImageFolder('../patches_images/patches_CRO_23072019/', transform=composed, target_transform=None, loader=read_image_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set dataset\n",
    "# patches = PatchesDataset(data_path='./patches_256x256.pkl', transform = composed)\n",
    "\n",
    "# Dataloader parameters\n",
    "batch_size = 8\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(patches)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# this train loader is to have random access\n",
    "loader = torch.utils.data.DataLoader(patches, batch_size=batch_size, \n",
    "                                            num_workers=10)\n",
    "train_loader = torch.utils.data.DataLoader(patches, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=10)\n",
    "validation_loader = torch.utils.data.DataLoader(patches, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "images, _ = next(iter(loader))\n",
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        ## encoder layers ##\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding = 1)\n",
    "        self.conv1a = nn.Conv2d(16, 16, 3, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "        \n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv1a(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        ## decode ##\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = torch.sigmoid(self.t_conv2(x))\n",
    "                \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.has_cudnn:\n",
    "    device = torch.device(\"cuda:{}\".format('0'))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "# F: sets model in evaluation mode. It has an effect in certain modules: e.g. Dropout or BatchNorm Layers\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 256, 256]             160\n",
      "            Conv2d-2         [-1, 16, 256, 256]           2,320\n",
      "         MaxPool2d-3         [-1, 16, 128, 128]               0\n",
      "            Conv2d-4          [-1, 4, 128, 128]             580\n",
      "         MaxPool2d-5            [-1, 4, 64, 64]               0\n",
      "   ConvTranspose2d-6         [-1, 16, 128, 128]             272\n",
      "   ConvTranspose2d-7          [-1, 1, 256, 256]              65\n",
      "================================================================\n",
      "Total params: 3,397\n",
      "Trainable params: 3,397\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 21.12\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 21.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(your_model, input_size=(channels, H, W))\n",
    "summary(model, input_size=(1, 256, 256), device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of using Sequential with OrderedDict\n",
    "# model = nn.Sequential(OrderedDict([\n",
    "#           ('conv1', nn.Conv2d(1,20,5)),\n",
    "#           ('relu1', nn.ReLU()),\n",
    "#           ('conv2', nn.Conv2d(20,64,5)),\n",
    "#           ('relu2', nn.ReLU())\n",
    "#         ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3eb26894ce4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# train the model #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# _ stands in for labels, here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# no need to flatten images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        # no need to flatten images\n",
    "        images, _ = data[0].to(device), data[1]\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images.to(device=device))\n",
    "# prep images for display\n",
    "images = images.cpu().numpy()\n",
    "\n",
    "# output is resized into a batch of iages\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.detach().cpu().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
